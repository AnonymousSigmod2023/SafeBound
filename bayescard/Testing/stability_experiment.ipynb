{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pareto\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('/home/ziniu.wzn/BayesCard')\n",
    "from Models.Bayescard_BN import Bayescard_BN\n",
    "from time import perf_counter\n",
    "from Evaluation.utils import parse_query\n",
    "from Evaluation.cardinality_estimation import parse_query_single_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_series(s, domain_size):\n",
    "    n_invalid = len(s[s>=domain_size])\n",
    "    s = s[s<domain_size]\n",
    "    s = np.floor(s)\n",
    "    new_s = np.random.randint(domain_size, size=n_invalid)\n",
    "    s = np.concatenate((s, new_s))\n",
    "    return np.random.permutation(s)\n",
    "    \n",
    "def data_generation(skew, domain_size, correlation, column_size, nrows=1000000):\n",
    "    data = np.zeros((column_size, nrows))\n",
    "    for i in range(column_size):\n",
    "        if i == 0:\n",
    "            s = np.random.randint(domain_size, size=nrows)\n",
    "            data[i,:] = s\n",
    "            continue\n",
    "        s = pareto.rvs(b=skew, scale=1, size=nrows)\n",
    "        s = discretize_series(s, domain_size)\n",
    "        if i == 1:\n",
    "            selected_cols = [0]\n",
    "        else:\n",
    "            #num_selected_cols = max(np.random.randint(int(np.ceil(i*0.1))), 1)\n",
    "            num_selected_cols = 1\n",
    "            selected_cols = np.random.permutation(i)[0:num_selected_cols]\n",
    "        idx = np.random.permutation(nrows)[0:int(nrows*correlation)]\n",
    "        if len(idx) != 0:\n",
    "            selected_data = data[selected_cols, :]\n",
    "            selected_data = np.ceil(np.mean(selected_data, axis=0))\n",
    "            s[idx] = selected_data[idx]\n",
    "        assert len(np.unique(s)) <= domain_size, \"invalid domain\"\n",
    "        data[i,:] = s\n",
    "        \n",
    "    data = pd.DataFrame(data=data.transpose(), columns=[f\"attr{i}\" for i in range(column_size)])\n",
    "    return data\n",
    "\n",
    "def query_generation(data, table_name, num_sample=200, p=0.8, nval_per_col=4, skip_zero_bit=6):\n",
    "    queries = []\n",
    "    cards = []\n",
    "    for i in range(num_sample):\n",
    "        query, card = generate_single_query(data, table_name, p, nval_per_col, skip_zero_bit)\n",
    "        while query is None:\n",
    "            query, card = generate_single_query(data, table_name, p, nval_per_col, skip_zero_bit)\n",
    "        queries.append(query)\n",
    "        cards.append(card)\n",
    "    return queries, cards\n",
    "\n",
    "def generate_single_query(df, table_name, p=0.8, nval_per_col=4, skip_zero_bit=6):\n",
    "    \"\"\"\n",
    "    p, nval_per_col, and skip_zero_bit are controlling the true cardinality size. As we know smaller true card \n",
    "    generally leads to larger q-error, which will bias the experimental result, so we use this to control the \n",
    "    true card to be similar for all experiments.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT COUNT(*) FROM {table_name} WHERE \"\n",
    "    execute_query = \"\"\n",
    "    column_names = df.columns\n",
    "    n_cols = 0\n",
    "    for i, col in enumerate(column_names):\n",
    "        a = np.random.choice([0,1], p=[p,1-p])\n",
    "        if a == 0:\n",
    "            index = np.random.choice(len(df), size=nval_per_col)\n",
    "            val = sorted(list(df[col].iloc[index]))\n",
    "            left_val = val[0]\n",
    "            right_val = val[-1]\n",
    "            if left_val == right_val:\n",
    "                sub_query = col + '==' + str(left_val) + ' and '\n",
    "                act_sub_query = col + ' = ' + str(left_val) + ' AND '\n",
    "            else:\n",
    "                if skip_zero_bit:\n",
    "                    left_val += skip_zero_bit\n",
    "                    right_val += skip_zero_bit\n",
    "                sub_query = str(left_val) + ' <= ' + col + ' <= ' + str(right_val) + ' and '\n",
    "                act_sub_query = col + ' >= ' + str(left_val) + ' AND ' + col + ' <= ' + str(right_val) + ' AND '\n",
    "            execute_query += sub_query\n",
    "            query += act_sub_query\n",
    "    if execute_query == \"\":\n",
    "        return None,  None\n",
    "    execute_query = execute_query[:-5]\n",
    "    query = query[:-5]\n",
    "    try:\n",
    "        card = len(df.query(execute_query))\n",
    "    except:\n",
    "        card = 0\n",
    "    if card==0:\n",
    "        return None, None\n",
    "    return query, card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(skew, domain_size, correlation, column_size, nrows=1000000, num_sample=200, \n",
    "              p=0.8, nval_per_col=4, skip_zero_bit=6, rows_to_use=10000, n_mcv=30, n_bins=70):\n",
    "    data = data_generation(skew, domain_size, correlation, column_size, nrows=nrows)\n",
    "    name = f\"toy_{skew}_{domain_size}_{correlation}_{column_size}\"\n",
    "    queries, cards = query_generation(data, name, num_sample, p, nval_per_col, skip_zero_bit)\n",
    "    BN = Bayescard_BN(name)\n",
    "    BN.build_from_data(data, sample_size=rows_to_use, n_mcv=n_mcv, n_bins=n_bins)\n",
    "    model_path = f'/home/ziniu.wzn/BN_checkpoints/synthetic/{name}.pkl'\n",
    "    pickle.dump(BN, open(model_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "    BN.infer_algo = \"exact-jit\"\n",
    "    BN.init_inference_method()\n",
    "    latencies = []\n",
    "    q_errors = []\n",
    "    for query_no, query_str in enumerate(queries):\n",
    "        query = parse_query_single_table(query_str.strip(), BN)\n",
    "        cardinality_true = cards[query_no]\n",
    "        card_start_t = perf_counter()\n",
    "        cardinality_predict = BN.query(query)\n",
    "        card_end_t = perf_counter()\n",
    "        latency_ms = (card_end_t - card_start_t) * 1000\n",
    "        if cardinality_predict == 0 and cardinality_true == 0:\n",
    "            q_error = 1.0\n",
    "        elif np.isnan(cardinality_predict) or cardinality_predict == 0:\n",
    "            cardinality_predict = 1\n",
    "            q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        elif cardinality_true == 0:\n",
    "            cardinality_true = 1\n",
    "            q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        else:\n",
    "            q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        latencies.append(latency_ms)\n",
    "        q_errors.append(q_error)\n",
    "    for i in [50, 90, 95, 99, 100]:\n",
    "        print(f\"q-error {i}% percentile is {np.percentile(q_errors, i)}\")\n",
    "    print(f\"average latency is {np.mean(latencies)} ms\")\n",
    "    return q_errors, latencies\n",
    "\n",
    "def run_all_experiment():\n",
    "    parameters_to_explore = {\n",
    "        \"skew\": [0.1, 0.3, 0.6, 1.0, 1.5, 2.0],\n",
    "        \"domain_size\": [10, 100, 500, 1000, 5000, 10000],\n",
    "        \"correlation\": [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        \"column_num\": [2, 5, 10, 50, 100, 200]\n",
    "    }\n",
    "    \n",
    "    print(\"runing experiment on varying skewness: [0.1, 0.3, 0.6, 1.0, 1.3, 1.6, 2.0]}\")\n",
    "    print(\"controlled parameters are: domain_size = 100, correlation = 0.4, column_num = 10\")\n",
    "    skew_qerrors = []\n",
    "    for skew in parameters_to_explore[\"skew\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing skewness = {skew}\")\n",
    "        q_errors, latencies = train_one(skew, 100, 0.4, 10, nrows=1000000)\n",
    "        skew_qerrors.append(np.asarray(q_errors))\n",
    "    skew_qerrors = np.stack(skew_qerrors)\n",
    "    np.save(\"skew_qerrors\", skew_qerrors)\n",
    "    \n",
    "    print(\"============================================================\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"runing experiment on varying domain_size: [10, 100, 1000, 10000]\")\n",
    "    print(\"controlled parameters are: skewness = 1.0, correlation = 0.4, column_num = 10\")\n",
    "    domain_size_qerrors = []\n",
    "    for domain_size in parameters_to_explore[\"domain_size\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing domain_size = {domain_size}\")\n",
    "        q_errors, latencies = train_one(1, domain_size, 0.4, 10, nrows=1000000)\n",
    "        domain_size_qerrors.append(np.asarray(q_errors))\n",
    "    domain_size_qerrors = np.stack(domain_size_qerrors)\n",
    "    np.save(\"domain_size_qerrors\", domain_size_qerrors)\n",
    "    \n",
    "    print(\"============================================================\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"runing experiment on varying correlation: [0, 0.2, 0.4, 0.6, 0.8, 1.0]\")\n",
    "    print(\"controlled parameters are: skewness = 1.0, domain_size = 100, column_num = 10\")\n",
    "    correlation_qerrors = []\n",
    "    for correlation in parameters_to_explore[\"correlation\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing correlation = {correlation}\")\n",
    "        q_errors, latencies = train_one(1, 100, correlation, 10, nrows=1000000)\n",
    "        correlation_qerrors.append(np.asarray(q_errors))\n",
    "    correlation_qerrors = np.stack(correlation_qerrors)\n",
    "    np.save(\"correlation_qerrors\", correlation_qerrors)\n",
    "    \n",
    "    print(\"============================================================\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"runing experiment on varying column_num: [2, 5, 10, 50, 100, 200]\")\n",
    "    print(\"controlled parameters are: skewness = 1.0, domain_size = 100, correlation = 0.4\")\n",
    "    column_num_qerrors = []\n",
    "    for column_num in parameters_to_explore[\"column_num\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing column_num = {column_num}\")\n",
    "        q_errors, latencies = train_one(1, 100, 0.4, column_num, nrows=1000000)\n",
    "        column_num_qerrors.append(np.asarray(q_errors))\n",
    "    column_num_qerrors = np.stack(column_num_qerrors)\n",
    "    np.save(\"column_num_qerrors\", column_num_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_explore = {\n",
    "        \"skew\": ([0.1, 0.3, 0.6, 1.0, 1.5, 2.0], [4,4,3,3,2,2]),\n",
    "        \"domain_size\": ([10, 100, 500, 1000, 5000, 10000], [4,4,4,4,4,4]),\n",
    "        \"correlation\": ([0, 0.2, 0.4, 0.6, 0.8, 1.0], [4,4,4,4,4,4]),\n",
    "        \"column_num\": ([2, 5, 10, 50, 100, 200], [6,6,4,2,0,0])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skew = parameters_to_explore[\"skew\"][0]\n",
    "print(f\"runing experiment on varying skewness: {skew}\")\n",
    "print(\"controlled parameters are: domain_size = 100, correlation = 0.4, column_num = 10\")\n",
    "skew_qerrors = []\n",
    "for i in range(len(skew)):\n",
    "    skew = parameters_to_explore[\"skew\"][0][i]\n",
    "    skip_zero_bit = parameters_to_explore[\"skew\"][1][i]\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing skewness = {skew}\")\n",
    "    q_errors, latencies = train_one(skew, 100, 0.4, 10, skip_zero_bit=skip_zero_bit)\n",
    "    skew_qerrors.append(np.asarray(q_errors))\n",
    "skew_qerrors = np.stack(skew_qerrors)\n",
    "np.save(\"skew_qerrors\", skew_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_size = parameters_to_explore[\"domain_size\"][0]\n",
    "print(f\"runing experiment on varying domain_size: {domain_size}\")\n",
    "print(\"controlled parameters are: skewness = 1.0, correlation = 0.4, column_num = 10\")\n",
    "domain_size_qerrors = []\n",
    "for i in range(len(domain_size)):\n",
    "    domain_size = parameters_to_explore[\"domain_size\"][0][i]\n",
    "    skip_zero_bit = parameters_to_explore[\"domain_size\"][1][i]\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing domain_size = {domain_size}\")\n",
    "    q_errors, latencies = train_one(1, domain_size, 0.4, 10, nrows=1000000, skip_zero_bit=skip_zero_bit)\n",
    "    domain_size_qerrors.append(np.asarray(q_errors))\n",
    "domain_size_qerrors = np.stack(domain_size_qerrors)\n",
    "np.save(\"domain_size_qerrors\", domain_size_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = parameters_to_explore[\"correlation\"][0]\n",
    "print(f\"runing experiment on varying correlation: {correlation}\")\n",
    "print(\"controlled parameters are: skewness = 1.0, domain_size = 100, column_num = 10\")\n",
    "correlation_qerrors = []\n",
    "for i in range(len(correlation)):\n",
    "    domain_size = parameters_to_explore[\"correlation\"][0][i]\n",
    "    skip_zero_bit = parameters_to_explore[\"correlation\"][1][i]\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing correlation = {correlation}\")\n",
    "    q_errors, latencies = train_one(1, 100, correlation, 10, nrows=1000000, skip_zero_bit=skip_zero_bit)\n",
    "    correlation_qerrors.append(np.asarray(q_errors))\n",
    "correlation_qerrors = np.stack(correlation_qerrors)\n",
    "np.save(\"correlation_qerrors\", correlation_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_num = parameters_to_explore[\"column_num\"][0]\n",
    "print(f\"runing experiment on varying column_num: {column_num}\")\n",
    "print(\"controlled parameters are: skewness = 1.0, domain_size = 100, correlation = 0.4\")\n",
    "column_num_qerrors = []\n",
    "p_list = [0.8, 0.8, 0.8, 0.2, 0.1, 0.05]\n",
    "for i in range(len(column_num)):\n",
    "    column_num = parameters_to_explore[\"column_num\"][0][i]\n",
    "    skip_zero_bit = parameters_to_explore[\"column_num\"][1][i]\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing column number = {column_num}\")\n",
    "    p = p_list[i]\n",
    "    q_errors, latencies = train_one(1, 100, 0.4, column_num, p=p, skip_zero_bit=skip_zero_bit, nrows=1000000)\n",
    "    column_num_qerrors.append(np.asarray(q_errors))\n",
    "column_num_qerrors = np.stack(column_num_qerrors)\n",
    "np.save(\"column_num_qerrors\", column_num_qerrors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
